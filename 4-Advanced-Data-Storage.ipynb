{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div width=50% style=\"display: block; margin: auto\">\n",
    "    <img src=\"figures/ucl-logo.svg\" width=100%>\n",
    "</div>\n",
    "\n",
    "\n",
    "### [UCL-ELEC0136 Data Acquisition and Processing Systems 2024]()\n",
    "University College London\n",
    "# Lab 4: Advanced Data Storage - Vector Databases and LLMs\n",
    "\n",
    "\n",
    "<hr width=70% style=\"float: left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT:** The content of this Notebook will not be evaluated in the final exam. The goal is to provide you with practical experience with the very trendy subject that LLMs are, for you to know how to use them, and provide you with enough to start your own LLM projects. \n",
    "\n",
    "This lab also serves as a good illustration of what this module is about: how data acquisition, storage, and processing all come together to create inteligent AI-driven applications.\n",
    "\n",
    "### Objectives\n",
    "* Perform CRUD operations on a Pinecone Vector Database.\n",
    "* Use Langchain to make queries to API accessed pre-trained LLM models (from Hugging Face and OpenAI).\n",
    "* Compare the performances of two pre-trained LLMs.\n",
    "* Use a Pinecone vector Database to perform Retrieval Augmentation of a pre-trained LLM, allowing it to both expend his knowledge base, and cite sources.\n",
    "\n",
    "### Outline\n",
    "\n",
    "This notebook has 3 parts:\n",
    "\n",
    "0. [Setting up](#0.-Setting-up)\n",
    "1. [CRUD operations on a vector database](#1-crud-operations-on-a-pinecone-vector-database)\n",
    "2. [Intro to LLMs and LangChain](#2.-Intro-to-LLMs-and-Langchain)\n",
    "3. [Retrieval Augmentation of a LLM using a vector database](#3.-Retrieval-Augmentation-of-a-LLM-using-a-vector-database)\n",
    "\n",
    "<hr width=70% style=\"float: left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setting up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- The assignment repository contains a `requirements.txt` file, make sure to install all the librairies with the correct versions listed in this file in your daps conda environment.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Create a Pinecone account and connect to your free-tier online vector database\n",
    "\n",
    "[Pinecone](https://www.pinecone.io) is a vector database service that helps developers build and deploy applications with high-performance similarity search and recommendation capabilities. It enables efficient storage and retrieval of vector data, making it easier to create personalized experiences and content recommendations in various applications, such as stable diffusion, LLMs chatbox, and many other AI applications.\n",
    "\n",
    "In this Notebook, we will use Pinecone's free tier to create and connect to a vector database, perform CRUD operations, and then use that database to power a LLM application.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "    \n",
    "- Follow the instructions in `pinecone_tutorial.pdf` to create a Pinecone account and get an API key for the free tier vector database. MAKE SURE TO KEEP A COPY OF YOUR API KEY.\n",
    "- Run the cell bellow to connect to your online vector database.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\daps\\Lib\\site-packages\\pinecone\\index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "# Run this cell (it may take a few seconds)\n",
    "import pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   change PINECONE_API_KEY with your pinecone API key and run the cell\n",
    "#\n",
    "###########################\n",
    "\n",
    "PINECONE_API_KEY = \"11b47bc9-b070-42a8-940a-6742ed0eb9e0\" #<--- TODO: your API key here \n",
    "\n",
    "pinecone.init(api_key=PINECONE_API_KEY, environment=\"gcp-starter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "    \n",
    "- Use the [`list_indexes`](https://docs.pinecone.io/reference/list_indexes) function to return the list of Pinecone indexes you have on your database (it should be empty).\n",
    "- If it isn't empty, use the [`delete_index`](https://docs.pinecone.io/reference/delete_index) function to delete any indexes you may have on your database.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   Check that your pinecone database does not contain any indexes, and delete them if there are any.\n",
    "#\n",
    "###########################\n",
    "\n",
    "# TODO : your code bellow\n",
    "\n",
    "pinecone.list_indexes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Create a HuggingFace account and generate a free API key\n",
    "\n",
    "**Note:** you do not need this step to do part. [# 1. CRUD opperations on a Vector Database](#1.-CRUD-opperations-on-a-Vector-Database).\n",
    "\n",
    "\n",
    "[Hugging Face ü§ó](https://huggingface.co) is a company and open-source platform that specializes in natural language processing (NLP) and provides tools, libraries, and pre-trained models for building and deploying NLP applications. Their most well-known product is the Transformers library, which offers access to a wide range of pre-trained NLP models, making it easier for developers to work with text-based tasks such as language translation, sentiment analysis, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "    \n",
    "- Follow the instructions in `huggingface_tutorial.pdf` to create a HuggingFace account and get an API key for the free tier vector database. MAKE SURE TO KEEP A COPY OF YOUR API KEY.\n",
    "- Run the cell bellow to set an environment variable to your API key. `langchain.HuggingFaceHub` will use this environment variable when sending a request to the Hugging Face server to authentificate the connection.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "###########################\n",
    "# Task: \n",
    "#   change HUGGING_FACE_API_KEY with your Hugging Face API key and run the cell to set the environment variable HUGGINGFACEHUB_API_TOKEN\n",
    "#\n",
    "###########################\n",
    "\n",
    "HUGGING_FACE_API_KEY = \"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\" #<--- TODO: your Hugging Face API key here \n",
    "\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = HUGGING_FACE_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3 Create an OpenAI account and generate a free API key\n",
    "\n",
    "**Note:** you do not need this step to do part. [1. CRUD opperations on a Vector Database](#1.-crud-operations-on-a-pinecone-vector-database) and part. [2.1 Hugging Face LLM](#211-initializing-the-llm).\n",
    "\n",
    "[OpenAI](https://openai.com) is an artificial intelligence (AI) research laboratory consisting of the for-profit OpenAI LP and its non-profit parent company, OpenAI Inc.\n",
    "\n",
    "OpenAI provides an [API](https://platform.openai.com/docs/overview) that allows developers to access and integrate the capabilities of OpenAI's language models into their own applications, products, or services. The OpenAI API is based on models like GPT and allows developers to make use of powerful natural language processing (NLP) functionalities.\n",
    "\n",
    "**Unlike Hugging Face, OpenAI is not open source, and the free tier of their API is only available for 3 months after the creation of a new account, and has a lot of restriction (for exemple, you are limited to 3 requests per minutes for each model). However, their models are state of the art and very powerful, which is why we will use them in this lab.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "    \n",
    "- Follow the instructions in `openai_tutorial.pdf` to create a HuggingFace account and get an API key for the free tier vector database. MAKE SURE TO KEEP A COPY OF YOUR API KEY.\n",
    "- Run the cell bellow to set an environment variable to your API key. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   change OPENAI_API_KEY with your OpenAI API key and run the cell to set the environment variable HUGGINGFACEHUB_API_TOKEN\n",
    "#\n",
    "###########################\n",
    "\n",
    "\n",
    "OPENAI_API_KEY = \"sk-jkn6hu5yFpgD1BtsbkUT3BlbFJcUSrDmdyAP01P1FM2jx8\" #<--- TODO: your OpenAI API key here \n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. CRUD operations on a Pinecone Vector Database\n",
    "\n",
    "*Source: https://docs.pinecone.io/docs/quickstart*\n",
    "\n",
    "\n",
    "In this part, we will familiarize ourselve with basic operations on a Pinecode Vector Database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>üë©‚Äçüíªüë®‚Äçüíª Optional action</b>\n",
    "\n",
    "- Very short video intro on Vector Databases: [Vector databases are so hot right now. What are they?](https://www.youtube.com/watch?v=klTvEwg3oJ4)\n",
    "- Short read: [Everything you need to know about Pinecone ‚Äì A Vector Database](https://www.packtpub.com/article-hub/everything-you-need-to-know-about-pinecone-a-vector-database).\n",
    "- In depth read: [What is a Vector Database & How Does it Work? Use Cases + Examples](https://www.pinecone.io/learn/vector-database/).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-heading alert-danger\" style=\"background-color: white; border: 2px solid; border-radius: 5px; color: #000; border-color:#AAA; padding: 10px\">\n",
    "    <b>üíé Tip</b>\n",
    "\n",
    "Once your connection to your Pinecone database is established (which should have been done in part. 0.1), you do not need to use `pinecone.init` anymore and can directly use the API requests.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 CRUD operations on a Vector Database - Create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "    \n",
    "- Use the [`create_index`](https://docs.pinecone.io/reference/create_index) function to create an index, name it `quickstart`, set the dimension to 8, and use the metric `euclidean`.\n",
    "- Check that your database contains an index called `quickstart`.\n",
    "- Use the [`describe_index`](https://docs.pinecone.io/reference/describe_index) function to get informations about the index `quickstart`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ApiException",
     "evalue": "(400)\nReason: Bad Request\nHTTP response headers: HTTPHeaderDict({'content-type': 'text/plain; charset=utf-8', 'Content-Length': '140', 'date': 'Wed, 15 Nov 2023 12:33:45 GMT', 'x-envoy-upstream-service-time': '1', 'server': 'envoy', 'Via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\nHTTP response body: Capacity Reached. Starter Projects support a single index. Create a new project to add more. Your Starter Project remains free post-upgrade.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mApiException\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\user\\OneDrive\\Documents\\Year4\\DPS\\4-advanced-data-storage-CurieN26\\4-Advanced-Data-Storage.ipynb Cell 22\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/Documents/Year4/DPS/4-advanced-data-storage-CurieN26/4-Advanced-Data-Storage.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m###########################\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/Documents/Year4/DPS/4-advanced-data-storage-CurieN26/4-Advanced-Data-Storage.ipynb#X30sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# Task: \u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/Documents/Year4/DPS/4-advanced-data-storage-CurieN26/4-Advanced-Data-Storage.ipynb#X30sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m#   Create an index called quickstart, check that it has been added to your Pinecone Vector DB, and get information about that index\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/Documents/Year4/DPS/4-advanced-data-storage-CurieN26/4-Advanced-Data-Storage.ipynb#X30sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/Documents/Year4/DPS/4-advanced-data-storage-CurieN26/4-Advanced-Data-Storage.ipynb#X30sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# TODO : your code bellow\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/Documents/Year4/DPS/4-advanced-data-storage-CurieN26/4-Advanced-Data-Storage.ipynb#X30sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m pinecone\u001b[39m.\u001b[39;49mcreate_index(\u001b[39m\"\u001b[39;49m\u001b[39mquickstart\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m8\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/Documents/Year4/DPS/4-advanced-data-storage-CurieN26/4-Advanced-Data-Storage.ipynb#X30sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m pinecone\u001b[39m.\u001b[39mlist_indexes()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/Documents/Year4/DPS/4-advanced-data-storage-CurieN26/4-Advanced-Data-Storage.ipynb#X30sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m pinecone\u001b[39m.\u001b[39mdescribe_index()\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\daps\\Lib\\site-packages\\pinecone\\manage.py:118\u001b[0m, in \u001b[0;36mcreate_index\u001b[1;34m(name, dimension, timeout, index_type, metric, replicas, shards, pods, pod_type, index_config, metadata_config, source_collection)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Creates a Pinecone index.\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \n\u001b[0;32m     84\u001b[0m \u001b[39m:param name: the name of the index.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39m:param timeout: Timeout for wait until index gets ready. If None, wait indefinitely; if >=0, time out after this many seconds; if -1, return immediately and do not wait. Default: None\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m api_instance \u001b[39m=\u001b[39m _get_api_instance()\n\u001b[1;32m--> 118\u001b[0m api_instance\u001b[39m.\u001b[39;49mcreate_index(create_request\u001b[39m=\u001b[39;49mCreateRequest(\n\u001b[0;32m    119\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m    120\u001b[0m     dimension\u001b[39m=\u001b[39;49mdimension,\n\u001b[0;32m    121\u001b[0m     index_type\u001b[39m=\u001b[39;49mindex_type,\n\u001b[0;32m    122\u001b[0m     metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[0;32m    123\u001b[0m     replicas\u001b[39m=\u001b[39;49mreplicas,\n\u001b[0;32m    124\u001b[0m     shards\u001b[39m=\u001b[39;49mshards,\n\u001b[0;32m    125\u001b[0m     pods\u001b[39m=\u001b[39;49mpods,\n\u001b[0;32m    126\u001b[0m     pod_type\u001b[39m=\u001b[39;49mpod_type,\n\u001b[0;32m    127\u001b[0m     index_config\u001b[39m=\u001b[39;49mindex_config \u001b[39mor\u001b[39;49;00m {},\n\u001b[0;32m    128\u001b[0m     metadata_config\u001b[39m=\u001b[39;49mmetadata_config,\n\u001b[0;32m    129\u001b[0m     source_collection\u001b[39m=\u001b[39;49msource_collection\n\u001b[0;32m    130\u001b[0m ))\n\u001b[0;32m    132\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_ready\u001b[39m():\n\u001b[0;32m    133\u001b[0m     status \u001b[39m=\u001b[39m _get_status(name)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\daps\\Lib\\site-packages\\pinecone\\core\\client\\api_client.py:776\u001b[0m, in \u001b[0;36mEndpoint.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    765\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    766\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" This method is invoked when endpoints are called\u001b[39;00m\n\u001b[0;32m    767\u001b[0m \u001b[39m    Example:\u001b[39;00m\n\u001b[0;32m    768\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    774\u001b[0m \n\u001b[0;32m    775\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 776\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcallable(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\daps\\Lib\\site-packages\\pinecone\\core\\client\\api\\index_operations_api.py:368\u001b[0m, in \u001b[0;36mIndexOperationsApi.__init__.<locals>.__create_index\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    364\u001b[0m kwargs[\u001b[39m'\u001b[39m\u001b[39m_check_return_type\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\n\u001b[0;32m    365\u001b[0m     \u001b[39m'\u001b[39m\u001b[39m_check_return_type\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    366\u001b[0m )\n\u001b[0;32m    367\u001b[0m kwargs[\u001b[39m'\u001b[39m\u001b[39m_host_index\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39m_host_index\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 368\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_with_http_info(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\daps\\Lib\\site-packages\\pinecone\\core\\client\\api_client.py:838\u001b[0m, in \u001b[0;36mEndpoint.call_with_http_info\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    834\u001b[0m     header_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_client\u001b[39m.\u001b[39mselect_header_content_type(\n\u001b[0;32m    835\u001b[0m         content_type_headers_list)\n\u001b[0;32m    836\u001b[0m     params[\u001b[39m'\u001b[39m\u001b[39mheader\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mContent-Type\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m header_list\n\u001b[1;32m--> 838\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapi_client\u001b[39m.\u001b[39;49mcall_api(\n\u001b[0;32m    839\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msettings[\u001b[39m'\u001b[39;49m\u001b[39mendpoint_path\u001b[39;49m\u001b[39m'\u001b[39;49m], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msettings[\u001b[39m'\u001b[39;49m\u001b[39mhttp_method\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    840\u001b[0m     params[\u001b[39m'\u001b[39;49m\u001b[39mpath\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    841\u001b[0m     params[\u001b[39m'\u001b[39;49m\u001b[39mquery\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    842\u001b[0m     params[\u001b[39m'\u001b[39;49m\u001b[39mheader\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    843\u001b[0m     body\u001b[39m=\u001b[39;49mparams[\u001b[39m'\u001b[39;49m\u001b[39mbody\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    844\u001b[0m     post_params\u001b[39m=\u001b[39;49mparams[\u001b[39m'\u001b[39;49m\u001b[39mform\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    845\u001b[0m     files\u001b[39m=\u001b[39;49mparams[\u001b[39m'\u001b[39;49m\u001b[39mfile\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    846\u001b[0m     response_type\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msettings[\u001b[39m'\u001b[39;49m\u001b[39mresponse_type\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    847\u001b[0m     auth_settings\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msettings[\u001b[39m'\u001b[39;49m\u001b[39mauth\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    848\u001b[0m     async_req\u001b[39m=\u001b[39;49mkwargs[\u001b[39m'\u001b[39;49m\u001b[39masync_req\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    849\u001b[0m     _check_type\u001b[39m=\u001b[39;49mkwargs[\u001b[39m'\u001b[39;49m\u001b[39m_check_return_type\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    850\u001b[0m     _return_http_data_only\u001b[39m=\u001b[39;49mkwargs[\u001b[39m'\u001b[39;49m\u001b[39m_return_http_data_only\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    851\u001b[0m     _preload_content\u001b[39m=\u001b[39;49mkwargs[\u001b[39m'\u001b[39;49m\u001b[39m_preload_content\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    852\u001b[0m     _request_timeout\u001b[39m=\u001b[39;49mkwargs[\u001b[39m'\u001b[39;49m\u001b[39m_request_timeout\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    853\u001b[0m     _host\u001b[39m=\u001b[39;49m_host,\n\u001b[0;32m    854\u001b[0m     collection_formats\u001b[39m=\u001b[39;49mparams[\u001b[39m'\u001b[39;49m\u001b[39mcollection_format\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\daps\\Lib\\site-packages\\pinecone\\core\\client\\api_client.py:413\u001b[0m, in \u001b[0;36mApiClient.call_api\u001b[1;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, async_req, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Makes the HTTP request (synchronous) and returns deserialized data.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[39mTo make an async_req request, set the async_req parameter.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    410\u001b[0m \u001b[39m    then the method will return the response directly.\u001b[39;00m\n\u001b[0;32m    411\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    412\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m async_req:\n\u001b[1;32m--> 413\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__call_api(resource_path, method,\n\u001b[0;32m    414\u001b[0m                            path_params, query_params, header_params,\n\u001b[0;32m    415\u001b[0m                            body, post_params, files,\n\u001b[0;32m    416\u001b[0m                            response_type, auth_settings,\n\u001b[0;32m    417\u001b[0m                            _return_http_data_only, collection_formats,\n\u001b[0;32m    418\u001b[0m                            _preload_content, _request_timeout, _host,\n\u001b[0;32m    419\u001b[0m                            _check_type)\n\u001b[0;32m    421\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool\u001b[39m.\u001b[39mapply_async(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__call_api, (resource_path,\n\u001b[0;32m    422\u001b[0m                                                method, path_params,\n\u001b[0;32m    423\u001b[0m                                                query_params,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    431\u001b[0m                                                _request_timeout,\n\u001b[0;32m    432\u001b[0m                                                _host, _check_type))\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\daps\\Lib\\site-packages\\pinecone\\core\\client\\api_client.py:207\u001b[0m, in \u001b[0;36mApiClient.__call_api\u001b[1;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[39mexcept\u001b[39;00m ApiException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    206\u001b[0m     e\u001b[39m.\u001b[39mbody \u001b[39m=\u001b[39m e\u001b[39m.\u001b[39mbody\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 207\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    209\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_response \u001b[39m=\u001b[39m response_data\n\u001b[0;32m    211\u001b[0m return_data \u001b[39m=\u001b[39m response_data\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\daps\\Lib\\site-packages\\pinecone\\core\\client\\api_client.py:200\u001b[0m, in \u001b[0;36mApiClient.__call_api\u001b[1;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[0;32m    196\u001b[0m     url \u001b[39m=\u001b[39m _host \u001b[39m+\u001b[39m resource_path\n\u001b[0;32m    198\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    199\u001b[0m     \u001b[39m# perform request and return response\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m     response_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    201\u001b[0m         method, url, query_params\u001b[39m=\u001b[39;49mquery_params, headers\u001b[39m=\u001b[39;49mheader_params,\n\u001b[0;32m    202\u001b[0m         post_params\u001b[39m=\u001b[39;49mpost_params, body\u001b[39m=\u001b[39;49mbody,\n\u001b[0;32m    203\u001b[0m         _preload_content\u001b[39m=\u001b[39;49m_preload_content,\n\u001b[0;32m    204\u001b[0m         _request_timeout\u001b[39m=\u001b[39;49m_request_timeout)\n\u001b[0;32m    205\u001b[0m \u001b[39mexcept\u001b[39;00m ApiException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    206\u001b[0m     e\u001b[39m.\u001b[39mbody \u001b[39m=\u001b[39m e\u001b[39m.\u001b[39mbody\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\daps\\Lib\\site-packages\\pinecone\\core\\client\\api_client.py:459\u001b[0m, in \u001b[0;36mApiClient.request\u001b[1;34m(self, method, url, query_params, headers, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrest_client\u001b[39m.\u001b[39mOPTIONS(url,\n\u001b[0;32m    452\u001b[0m                                     query_params\u001b[39m=\u001b[39mquery_params,\n\u001b[0;32m    453\u001b[0m                                     headers\u001b[39m=\u001b[39mheaders,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    456\u001b[0m                                     _request_timeout\u001b[39m=\u001b[39m_request_timeout,\n\u001b[0;32m    457\u001b[0m                                     body\u001b[39m=\u001b[39mbody)\n\u001b[0;32m    458\u001b[0m \u001b[39melif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPOST\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 459\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrest_client\u001b[39m.\u001b[39;49mPOST(url,\n\u001b[0;32m    460\u001b[0m                                  query_params\u001b[39m=\u001b[39;49mquery_params,\n\u001b[0;32m    461\u001b[0m                                  headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    462\u001b[0m                                  post_params\u001b[39m=\u001b[39;49mpost_params,\n\u001b[0;32m    463\u001b[0m                                  _preload_content\u001b[39m=\u001b[39;49m_preload_content,\n\u001b[0;32m    464\u001b[0m                                  _request_timeout\u001b[39m=\u001b[39;49m_request_timeout,\n\u001b[0;32m    465\u001b[0m                                  body\u001b[39m=\u001b[39;49mbody)\n\u001b[0;32m    466\u001b[0m \u001b[39melif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPUT\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    467\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrest_client\u001b[39m.\u001b[39mPUT(url,\n\u001b[0;32m    468\u001b[0m                                 query_params\u001b[39m=\u001b[39mquery_params,\n\u001b[0;32m    469\u001b[0m                                 headers\u001b[39m=\u001b[39mheaders,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    472\u001b[0m                                 _request_timeout\u001b[39m=\u001b[39m_request_timeout,\n\u001b[0;32m    473\u001b[0m                                 body\u001b[39m=\u001b[39mbody)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\daps\\Lib\\site-packages\\pinecone\\core\\client\\rest.py:271\u001b[0m, in \u001b[0;36mRESTClientObject.POST\u001b[1;34m(self, url, headers, query_params, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mPOST\u001b[39m(\u001b[39mself\u001b[39m, url, headers\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, query_params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, post_params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    270\u001b[0m          body\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, _preload_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, _request_timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m--> 271\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(\u001b[39m\"\u001b[39;49m\u001b[39mPOST\u001b[39;49m\u001b[39m\"\u001b[39;49m, url,\n\u001b[0;32m    272\u001b[0m                         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    273\u001b[0m                         query_params\u001b[39m=\u001b[39;49mquery_params,\n\u001b[0;32m    274\u001b[0m                         post_params\u001b[39m=\u001b[39;49mpost_params,\n\u001b[0;32m    275\u001b[0m                         _preload_content\u001b[39m=\u001b[39;49m_preload_content,\n\u001b[0;32m    276\u001b[0m                         _request_timeout\u001b[39m=\u001b[39;49m_request_timeout,\n\u001b[0;32m    277\u001b[0m                         body\u001b[39m=\u001b[39;49mbody)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\daps\\Lib\\site-packages\\pinecone\\core\\client\\rest.py:230\u001b[0m, in \u001b[0;36mRESTClientObject.request\u001b[1;34m(self, method, url, query_params, headers, body, post_params, _preload_content, _request_timeout)\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m500\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m r\u001b[39m.\u001b[39mstatus \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m599\u001b[39m:\n\u001b[0;32m    228\u001b[0m         \u001b[39mraise\u001b[39;00m ServiceException(http_resp\u001b[39m=\u001b[39mr)\n\u001b[1;32m--> 230\u001b[0m     \u001b[39mraise\u001b[39;00m ApiException(http_resp\u001b[39m=\u001b[39mr)\n\u001b[0;32m    232\u001b[0m \u001b[39mreturn\u001b[39;00m r\n",
      "\u001b[1;31mApiException\u001b[0m: (400)\nReason: Bad Request\nHTTP response headers: HTTPHeaderDict({'content-type': 'text/plain; charset=utf-8', 'Content-Length': '140', 'date': 'Wed, 15 Nov 2023 12:33:45 GMT', 'x-envoy-upstream-service-time': '1', 'server': 'envoy', 'Via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\nHTTP response body: Capacity Reached. Starter Projects support a single index. Create a new project to add more. Your Starter Project remains free post-upgrade.\n"
     ]
    }
   ],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   Create an index called quickstart, check that it has been added to your Pinecone Vector DB, and get information about that index\n",
    "#\n",
    "###########################\n",
    "\n",
    "# TODO : your code bellow\n",
    "pinecone.create_index(\"quickstart\", 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/n\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "IndexDescription(name='quickstart', metric='cosine', replicas=1, dimension=8.0, shards=1, pods=1, pod_type='starter', status={'ready': True, 'state': 'Ready'}, metadata_config=None, source_collection='')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pinecone.list_indexes()\n",
    "print(\"/n\")\n",
    "pinecone.describe_index(\"quickstart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone.delete_index(\"quickstart\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "    \n",
    "- Use the [`Index`](https://docs.pinecone.io/docs/python-client#index) class to contruct an `Index` object from the index `quickstart`. \n",
    "- Use the [`upsert`](https://docs.pinecone.io/docs/python-client#indexupsert) method to push the 5 vectors in the cell bellow to the index `quickstart`.\n",
    "- Use the [`describe_index_stats`](https://docs.pinecone.io/docs/python-client#indexdescribe_index_stats) method to get statistics about the index's contents.\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-heading alert-danger\" style=\"background-color: white; border: 2px solid; border-radius: 5px; color: #000; border-color:#AAA; padding: 10px\">\n",
    "    <b>üíé Tip</b>\n",
    "\n",
    "Wait for a few seconds after you use `upsert` before querying the index with `describe_index_stats` as the data needs a bit of time to be saved.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   Construct an Index object and use it to push the 5 vectors in data to your index, and get statistics about the index\n",
    "#\n",
    "###########################\n",
    "\n",
    "data = [\n",
    "    (\"A\", [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1], {\"genre\": \"comedy\", \"year\": 2020}),\n",
    "    (\"B\", [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2], {\"genre\": \"documentary\", \"year\": 2019}),\n",
    "    (\"C\", [0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3], {\"genre\": \"comedy\", \"year\": 2019}),\n",
    "    (\"D\", [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4], {\"genre\": \"drama\"}),\n",
    "    (\"E\", [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], {\"genre\": \"drama\"})\n",
    "    ]\n",
    "\n",
    "\n",
    "# TODO : your code bellow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 CRUD operations on a Vector Database - Read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "    \n",
    "- Use the [`query`](https://docs.pinecone.io/docs/python-client#indexquery) method to search for the 3 closest vectors to the `target_vector` in the index `quickstart`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   find the 3 nearest vectors to the target_vector\n",
    "#\n",
    "###########################\n",
    "target_vector = [0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3]\n",
    "\n",
    "# TODO : your code bellow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "    \n",
    "- Use the [`query`](https://docs.pinecone.io/docs/python-client#indexquery) method to search for the 3 closest vectors to the `target_vector` the index `quickstart` that are dramas.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   find the 3 nearest vectors to the target_vector that are dramas\n",
    "#\n",
    "###########################\n",
    "\n",
    "# TODO : your code bellow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 CRUD operations on a Vector Database - Update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to change the values associated to vectors A and D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "print(index.query(\n",
    "  id = \"A\",\n",
    "  top_k = 1,\n",
    "  include_values = True,\n",
    "    include_metadata = True\n",
    "))\n",
    "\n",
    "print(index.query(\n",
    "  id = \"D\",\n",
    "  top_k = 1,\n",
    "  include_values = True,\n",
    "  include_metadata = True\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "    \n",
    "- Use the [`update`](https://docs.pinecone.io/docs/python-client#indexupdate) method to do the following updates:\n",
    "    - Replace all the values of `A` by `0.6`, change the genre to `action-comedy`.\n",
    "    - Replace all the values of `D` by `-0.4`, add a `year` field in the metadata and set it to `2019`.\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-heading alert-danger\" style=\"background-color: white; border: 2px solid; border-radius: 5px; color: #000; border-color:#AAA; padding: 10px\">\n",
    "    <b>üíé Tip</b>\n",
    "\n",
    "Wait for a few seconds after you use `update` before querying the index as the data needs a bit of time to be saved.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   Update vectors A and D as indicated above.\n",
    "#\n",
    "###########################\n",
    "\n",
    "\n",
    "# TODO : your code bellow\n",
    "\n",
    "\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "print(index.query(\n",
    "  id = \"A\",\n",
    "  top_k = 1,\n",
    "  include_values = True,\n",
    "  include_metadata = True\n",
    "))\n",
    "\n",
    "print(index.query(\n",
    "  id = \"D\",\n",
    "  top_k = 1,\n",
    "  include_values = True,\n",
    "  include_metadata = True\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 CRUD operations on a Vector Database - Delete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "    \n",
    "- Use the [`delete`](https://docs.pinecone.io/docs/python-client#indexdelete) method to delete the vectors `B` and `C`.\n",
    "- Check with `describe_index_stats` that your index now contains 3 vectors\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-heading alert-danger\" style=\"background-color: white; border: 2px solid; border-radius: 5px; color: #000; border-color:#AAA; padding: 10px\">\n",
    "    <b>üíé Tip</b>\n",
    "\n",
    "Wait for a few seconds after you use `upsert` before querying the index with `describe_index_stats` as the data needs a bit of time to be saved.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   Delete vectors B and C from the index.\n",
    "#\n",
    "###########################\n",
    "\n",
    "\n",
    "# TODO : your code bellow\n",
    "\n",
    "\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "    \n",
    "- Use the [`delete_index`](https://docs.pinecone.io/reference/delete_index) function to delete the index `quickstart`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   delete the index quickstart from your Pinecode Vector Database\n",
    "#\n",
    "###########################\n",
    "\n",
    "# TODO : your code bellow\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Intro to LLMs and LangChain\n",
    "\n",
    "**Context:**\n",
    "\n",
    "* **Large language models (LLMs)** are a class of artificial intelligence models that have been trained on vast amounts of text data to understand and generate human-like text. These models are based on deep learning techniques, such as neural networks, and have many parameters, often numbering in the hundreds of millions or even billions. Some well-known examples include OpenAI's GPT-3 and GPT-4, and Google's BERT and T5 models.\n",
    "\n",
    "* \" [**LangChain**](https://python.langchain.com/docs/get_started/introduction) is an open source framework that lets software developers working with artificial intelligence (AI) and its machine learning subset combine large language models with other external components to develop LLM-powered applications. The goal of LangChain is to link powerful LLMs, such as OpenAI's GPT-3.5 and GPT-4, to an array of external data sources to create and reap the benefits of natural language processing (NLP) applications. \" - [Source](https://www.techtarget.com/searchenterpriseai/definition/LangChain#:~:text=LangChain%20is%20an%20open%20source,to%20develop%20LLM%2Dpowered%20applications.)\n",
    "\n",
    "\n",
    "\n",
    "**Although we are doing these operations in a Jupyter Notebook, the exact same code can be used to program server-hosted web applications that could perform real-world tasks.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>üë©‚Äçüíªüë®‚Äçüíª Optional action</b>\n",
    "\n",
    "- In depth read: [LangChain AI Handbook](https://www.pinecone.io/learn/series/langchain/).\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "In this part, we will use LangChain to deploy two pre-trained LLMs, one from Hugging Face Hub, and one from OpenAI's API.\n",
    "\n",
    "*Source: [https://www.pinecone.io/learn/series/langchain/langchain-intro/](https://www.pinecone.io/learn/series/langchain/langchain-intro/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Hugging Face LLM\n",
    "\n",
    "Let's use a pre-trained Google language model, [flan-t5-xxl](https://huggingface.co/google/flan-t5-xxl), hosted on the Hugging Face Hub, that we will access for free through Hugging Face's API. More specifically, we will use the HuggingFaceHub module of the langchain library, which will query Hugging Face's API for us.\n",
    "\n",
    "### 2.1.1 Initializing the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Run the cell below to initialize the connection to the LLM.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "\n",
    "from langchain import HuggingFaceHub\n",
    "\n",
    "# initialize Hub LLM\n",
    "hub_llm = HuggingFaceHub(\n",
    "        repo_id='google/flan-t5-xxl',\n",
    "    model_kwargs={'temperature':1e-2} #Best temperature found: 1e-2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Use LangChain to ask a question to the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do to query a LLM is to create a **prompt template**. A prompt template contains instructions to generate a prompt in a reproductible way. It contains a text string (\"the template\"), that can take in a set of parameters from the end user and generates a prompt (the input variables).\n",
    "\n",
    "For example: \n",
    "\n",
    "* We want a LLM model to tell what language a sentence is written in, then an appropriate prompt template would be: \n",
    "    * `\"What language is the sentense \"{sentence}\" written in?\"`\n",
    "    * Here, `sentence` is the only input variable. \n",
    "\n",
    "<br/>\n",
    "\n",
    "* We want a LLM model to generate jokes about a topic, while specifying what type of jokes, then an appropriate prompt template would be: \n",
    "    * `\"Make a {type} of joke about {subject}\"\"`\n",
    "    * Here, `type` and `subject` are the input variables.\n",
    "\n",
    "<br/>\n",
    "\n",
    "\n",
    "**Note:** `google/flan-t5-xxl` is a small LLM, best suited to give short answers. Although it is enough for us to explore LLMs today, for a real application bigger models would be better suited.\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Using [langchain.PromptTemplate](https://api.python.langchain.com/en/latest/prompts/langchain.prompts.prompt.PromptTemplate.html), create a prompt template with an input variable called `name` that asks the language model to give the date of birth of a historical figure. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   Create a PromptTemplate with an input variable called `name` that asks the language model to give the date of birth of a historical figure.\n",
    "#\n",
    "###########################\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "# TODO : your code bellow\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Using langchain.LLMChain, create a chain to run the prompt by our LLM.\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-heading alert-danger\" style=\"background-color: white; border: 2px solid; border-radius: 5px; color: #000; border-color:#AAA; padding: 10px\">\n",
    "    <b>üíé Tip</b>\n",
    "\n",
    "An LLMChain consists of a PromptTemplate and a language model (either an LLM or chat model). It formats the prompt template using the input key values provided, passes the formatted string to LLM and returns the LLM output.\n",
    "\n",
    "**Example:** \n",
    "llm_chain = LLMChain(\n",
    "    prompt=your_PromptTemplate_template,\n",
    "    llm=your_llm\n",
    ")\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   Create a LLMchain with your prompt and your hub_llm\n",
    "#\n",
    "###########################\n",
    "\n",
    "from langchain import LLMChain\n",
    "\n",
    "# TODO : your code bellow\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Use `LLMChain.run()` or `LLMChain.predict()` to ask for the date of birth of Napoleon.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   Asks the language model to give the date of birth of a historical figure.\n",
    "#\n",
    "###########################\n",
    "\n",
    "# TODO : your code bellow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SPOILER:** Well, that's not amazing. The LLM understood we wanted a date, and we even got something close to Napoleon's real date of birth, but the results is wrong. This is because this LLM is small and not adapted to this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 OpenAI LLM\n",
    "\n",
    "In this part, we will task a more powerfull openAI LLM called [`text-davinci-003`](https://platform.openai.com/docs/models/gpt-3), a variant of GPT3,to perform the same task we instructed the Hugging Face hosted model, and see if we get better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Initializing the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Run the cell bellow to innitialize the connection to the LLM.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "davinci = OpenAI(model_name='text-davinci-003', openai_api_key =  OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Use LangChain to ask a question to the LLM\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Using langchain.LLMChain, create a chain to run the same prompt as in part. 2.1.2 by our OpenAI LLM.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   Create a LLMchain with your prompt and your hub_llm\n",
    "#\n",
    "###########################\n",
    "\n",
    "# TODO : your code bellow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Use `LLMChain.run()` or `LLMChain.predict()` to ask for the date of birth of Napoleon.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   Asks the language model to give the date of birth of a historical figure.\n",
    "#\n",
    "###########################\n",
    "\n",
    "# TODO : your code bellow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SPOILER:** Alright that's better, Napoleon was indeed born of August 15, 1769. Let's ask something more complex and recent and see how our LLM does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Using [langchain.PromptTemplate](https://api.python.langchain.com/en/latest/prompts/langchain.prompts.prompt.PromptTemplate.html), create a new prompt template with an input variable called `year` that asks the language model to give the winner of the FIFA world cup on a given year.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   Create a PromptTemplate with an input variable called `year` that asks the language model to give the winner of the FIFA world cup of that year.\n",
    "###########################\n",
    "\n",
    "# TODO : your code bellow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Using langchain.LLMChain, create a chain to run the new prompt template by our OpenAI LLM.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   Create a LLMchain with your prompt and your hub_llm\n",
    "#\n",
    "###########################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Use `LLMChain.run()` or `LLMChain.predict()` to ask for the winner of the 2022 FIFA world cup.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   Asks the language model to ask for the winner of the 2022 FIFA world cup\n",
    "#\n",
    "###########################\n",
    "\n",
    "# TODO : your code bellow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LLM doesn't know what happened in 2022 because it was trained before that year. How can we update the LLM's knowledge to make it up to date? With Retrieval augmentation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Retrieval Augmentation of a LLM using a Vector Database\n",
    "\n",
    "**WARNING:** Some of the cells in this part may take some time to run as we are working with a lot of data.\n",
    "\n",
    "The most powerful LLMs in the world have no idea about recent world events, nor can they cite sources. In general a LLM will only have knowledge about what it has been exposed to during training. For LLMs, the world exists as a static snapshot of the world as it was within their training data. \n",
    "\n",
    "A solution to this problem is **retrieval augmentation**: we retrieve relevant information from an external knowledge base and give that information to our LLM.\n",
    "\n",
    "*Source: https://docs.pinecone.io/docs/langchain#retrieval-augmentation-in-langchain*\n",
    "\n",
    "\n",
    "<div width=50% style=\"display: block; margin: auto\">\n",
    "    <img src=\"figures/augmentation.png\" width=70%>\n",
    "</div>\n",
    "\n",
    "To perform **retrieval augmentation**, we will embed data with an **embedding model**, store the embedded data into a **vector database index**, and create a **LangChain vectorstore** that will use the index and the embedding model to find relevant information to the prompt sent by the user, and feed the most relevant results found in the database to the **LLM** to provide it with context and sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Building a knowledge base with Vector Embedding \n",
    "\n",
    "Source: [Creating the knowledge base](https://www.pinecone.io/learn/series/langchain/langchain-retrieval-augmentation/#Creating-the-Knowledge-Base)\n",
    "\n",
    "This part concists in taking a dataset of relevant content we want to augment our LLM with (it could be code documentation for an LLM that needs to help write code, company documents for an internal chatbot...), process it, and embedded it into vectors. \n",
    "\n",
    "You can look [here](https://www.pinecone.io/learn/series/langchain/langchain-retrieval-augmentation/#Creating-the-Knowledge-Base) fore the full processs, which you would have to do if you wanted to augment your LLM for a specific use-case.\n",
    "\n",
    "**For the sake of simplicity, we will use a pre-embembeded dataset from pinecone_dataset and upload it to a Pinecone Vector database. Unfortunatly, this dataset doesn't contain data on the 2022 FIFA world cup, but the process used here can be applied with other datasets, giving you an idea of the process should you want to use it for personal projects.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Run the cell bellow to import and process the pre-embedded data we will use for retrieval augmentation\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_request non-retriable exception: Invalid bucket name: 'pinecone-datasets-dev\\wikipedia-simple-text-embedding-ada-002-100K', 400\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\user\\anaconda3\\envs\\daps\\Lib\\site-packages\\gcsfs\\retry.py\", line 123, in retry_request\n",
      "    return await func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\anaconda3\\envs\\daps\\Lib\\site-packages\\gcsfs\\core.py\", line 430, in _request\n",
      "    validate_response(status, contents, path, args)\n",
      "  File \"c:\\Users\\user\\anaconda3\\envs\\daps\\Lib\\site-packages\\gcsfs\\retry.py\", line 110, in validate_response\n",
      "    raise HttpError(error)\n",
      "gcsfs.retry.HttpError: Invalid bucket name: 'pinecone-datasets-dev\\wikipedia-simple-text-embedding-ada-002-100K', 400\n"
     ]
    },
    {
     "ename": "HttpError",
     "evalue": "Invalid bucket name: 'pinecone-datasets-dev\\wikipedia-simple-text-embedding-ada-002-100K', 400",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\user\\OneDrive\\Documents\\Year4\\DPS\\4-advanced-data-storage-CurieN26\\4-Advanced-Data-Storage.ipynb Cell 75\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/Documents/Year4/DPS/4-advanced-data-storage-CurieN26/4-Advanced-Data-Storage.ipynb#Y133sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpinecone_datasets\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/Documents/Year4/DPS/4-advanced-data-storage-CurieN26/4-Advanced-Data-Storage.ipynb#Y133sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# We import a dataset\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/Documents/Year4/DPS/4-advanced-data-storage-CurieN26/4-Advanced-Data-Storage.ipynb#Y133sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m dataset \u001b[39m=\u001b[39m pinecone_datasets\u001b[39m.\u001b[39;49mload_dataset(\u001b[39m'\u001b[39;49m\u001b[39mwikipedia-simple-text-embedding-ada-002-100K\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/Documents/Year4/DPS/4-advanced-data-storage-CurieN26/4-Advanced-Data-Storage.ipynb#Y133sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# We drop sparse_values as they are not needed for this example\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/Documents/Year4/DPS/4-advanced-data-storage-CurieN26/4-Advanced-Data-Storage.ipynb#Y133sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m dataset\u001b[39m.\u001b[39mdocuments\u001b[39m.\u001b[39mdrop([\u001b[39m'\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m'\u001b[39m], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\daps\\Lib\\site-packages\\pinecone_datasets\\public.py:59\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(dataset_id, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDataset \u001b[39m\u001b[39m{\u001b[39;00mdataset_id\u001b[39m}\u001b[39;00m\u001b[39m not found in catalog\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     58\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m Dataset\u001b[39m.\u001b[39;49mfrom_catalog(dataset_id, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\daps\\Lib\\site-packages\\pinecone_datasets\\dataset.py:96\u001b[0m, in \u001b[0;36mDataset.from_catalog\u001b[1;34m(cls, dataset_id, catalog_base_path, **kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m catalog_base_path \u001b[39m=\u001b[39m (\n\u001b[0;32m     91\u001b[0m     catalog_base_path\n\u001b[0;32m     92\u001b[0m     \u001b[39mif\u001b[39;00m catalog_base_path\n\u001b[0;32m     93\u001b[0m     \u001b[39melse\u001b[39;00m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mDATASETS_CATALOG_BASEPATH\u001b[39m\u001b[39m\"\u001b[39m, cfg\u001b[39m.\u001b[39mStorage\u001b[39m.\u001b[39mendpoint)\n\u001b[0;32m     94\u001b[0m )\n\u001b[0;32m     95\u001b[0m dataset_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(catalog_base_path, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mdataset_id\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 96\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(dataset_path\u001b[39m=\u001b[39;49mdataset_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\daps\\Lib\\site-packages\\pinecone_datasets\\dataset.py:196\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[1;34m(self, dataset_path, **kwargs)\u001b[0m\n\u001b[0;32m    194\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fs \u001b[39m=\u001b[39m get_cloud_fs(endpoint, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    195\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_path \u001b[39m=\u001b[39m dataset_path\n\u001b[1;32m--> 196\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fs\u001b[39m.\u001b[39;49mexists(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_path):\n\u001b[0;32m    197\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m    198\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mDataset does not exist. Please check the path or dataset_id\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    199\u001b[0m         )\n\u001b[0;32m    200\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\daps\\Lib\\site-packages\\fsspec\\asyn.py:118\u001b[0m, in \u001b[0;36msync_wrapper.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    116\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    117\u001b[0m     \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m obj \u001b[39mor\u001b[39;00m args[\u001b[39m0\u001b[39m]\n\u001b[1;32m--> 118\u001b[0m     \u001b[39mreturn\u001b[39;00m sync(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloop, func, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\daps\\Lib\\site-packages\\fsspec\\asyn.py:103\u001b[0m, in \u001b[0;36msync\u001b[1;34m(loop, func, timeout, *args, **kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[39mraise\u001b[39;00m FSTimeoutError \u001b[39mfrom\u001b[39;00m \u001b[39mreturn_result\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(return_result, \u001b[39mBaseException\u001b[39;00m):\n\u001b[1;32m--> 103\u001b[0m     \u001b[39mraise\u001b[39;00m return_result\n\u001b[0;32m    104\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    105\u001b[0m     \u001b[39mreturn\u001b[39;00m return_result\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\daps\\Lib\\site-packages\\fsspec\\asyn.py:56\u001b[0m, in \u001b[0;36m_runner\u001b[1;34m(event, coro, result, timeout)\u001b[0m\n\u001b[0;32m     54\u001b[0m     coro \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mwait_for(coro, timeout\u001b[39m=\u001b[39mtimeout)\n\u001b[0;32m     55\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 56\u001b[0m     result[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m coro\n\u001b[0;32m     57\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m ex:\n\u001b[0;32m     58\u001b[0m     result[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m ex\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\daps\\Lib\\site-packages\\fsspec\\asyn.py:667\u001b[0m, in \u001b[0;36mAsyncFileSystem._exists\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    665\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39m_exists\u001b[39m(\u001b[39mself\u001b[39m, path):\n\u001b[0;32m    666\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m         \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info(path)\n\u001b[0;32m    668\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    669\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\daps\\Lib\\site-packages\\gcsfs\\core.py:929\u001b[0m, in \u001b[0;36mGCSFileSystem._info\u001b[1;34m(self, path, generation, **kwargs)\u001b[0m\n\u001b[0;32m    927\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m path:\n\u001b[0;32m    928\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 929\u001b[0m         out \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m\"\u001b[39m\u001b[39mGET\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mb/\u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, json_out\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    930\u001b[0m         out\u001b[39m.\u001b[39mupdate(size\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdirectory\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    931\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[0;32m    932\u001b[0m         \u001b[39m# GET bucket failed, try ls; will have no metadata\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\daps\\Lib\\site-packages\\gcsfs\\core.py:437\u001b[0m, in \u001b[0;36mGCSFileSystem._call\u001b[1;34m(self, method, path, json_out, info_out, *args, **kwargs)\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\n\u001b[0;32m    434\u001b[0m     \u001b[39mself\u001b[39m, method, path, \u001b[39m*\u001b[39margs, json_out\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, info_out\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    435\u001b[0m ):\n\u001b[0;32m    436\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmethod\u001b[39m.\u001b[39mupper()\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00margs\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mkwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mheaders\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 437\u001b[0m     status, headers, info, contents \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_request(\n\u001b[0;32m    438\u001b[0m         method, path, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    439\u001b[0m     )\n\u001b[0;32m    440\u001b[0m     \u001b[39mif\u001b[39;00m json_out:\n\u001b[0;32m    441\u001b[0m         \u001b[39mreturn\u001b[39;00m json\u001b[39m.\u001b[39mloads(contents)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\daps\\Lib\\site-packages\\decorator.py:221\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m kwsyntax:\n\u001b[0;32m    220\u001b[0m     args, kw \u001b[39m=\u001b[39m fix(args, kw, sig)\n\u001b[1;32m--> 221\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m caller(func, \u001b[39m*\u001b[39m(extras \u001b[39m+\u001b[39m args), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\daps\\Lib\\site-packages\\gcsfs\\retry.py:158\u001b[0m, in \u001b[0;36mretry_request\u001b[1;34m(func, retries, *args, **kwargs)\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m    157\u001b[0m logger\u001b[39m.\u001b[39mexception(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m non-retriable exception: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 158\u001b[0m \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\daps\\Lib\\site-packages\\gcsfs\\retry.py:123\u001b[0m, in \u001b[0;36mretry_request\u001b[1;34m(func, retries, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[39mif\u001b[39;00m retry \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    122\u001b[0m         \u001b[39mawait\u001b[39;00m asyncio\u001b[39m.\u001b[39msleep(\u001b[39mmin\u001b[39m(random\u001b[39m.\u001b[39mrandom() \u001b[39m+\u001b[39m \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m (retry \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m), \u001b[39m32\u001b[39m))\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    124\u001b[0m \u001b[39mexcept\u001b[39;00m (\n\u001b[0;32m    125\u001b[0m     HttpError,\n\u001b[0;32m    126\u001b[0m     requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mRequestException,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    129\u001b[0m     aiohttp\u001b[39m.\u001b[39mclient_exceptions\u001b[39m.\u001b[39mClientError,\n\u001b[0;32m    130\u001b[0m ) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    131\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    132\u001b[0m         \u001b[39misinstance\u001b[39m(e, HttpError)\n\u001b[0;32m    133\u001b[0m         \u001b[39mand\u001b[39;00m e\u001b[39m.\u001b[39mcode \u001b[39m==\u001b[39m \u001b[39m400\u001b[39m\n\u001b[0;32m    134\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mrequester pays\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m e\u001b[39m.\u001b[39mmessage\n\u001b[0;32m    135\u001b[0m     ):\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\daps\\Lib\\site-packages\\gcsfs\\core.py:430\u001b[0m, in \u001b[0;36mGCSFileSystem._request\u001b[1;34m(self, method, path, headers, json, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    427\u001b[0m info \u001b[39m=\u001b[39m r\u001b[39m.\u001b[39mrequest_info  \u001b[39m# for debug only\u001b[39;00m\n\u001b[0;32m    428\u001b[0m contents \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m r\u001b[39m.\u001b[39mread()\n\u001b[1;32m--> 430\u001b[0m validate_response(status, contents, path, args)\n\u001b[0;32m    431\u001b[0m \u001b[39mreturn\u001b[39;00m status, headers, info, contents\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\daps\\Lib\\site-packages\\gcsfs\\retry.py:110\u001b[0m, in \u001b[0;36mvalidate_response\u001b[1;34m(status, content, path, args)\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBad Request: \u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    109\u001b[0m \u001b[39melif\u001b[39;00m error:\n\u001b[1;32m--> 110\u001b[0m     \u001b[39mraise\u001b[39;00m HttpError(error)\n\u001b[0;32m    111\u001b[0m \u001b[39melif\u001b[39;00m status:\n\u001b[0;32m    112\u001b[0m     \u001b[39mraise\u001b[39;00m HttpError({\u001b[39m\"\u001b[39m\u001b[39mcode\u001b[39m\u001b[39m\"\u001b[39m: status, \u001b[39m\"\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m\"\u001b[39m: msg})  \u001b[39m# text-like\u001b[39;00m\n",
      "\u001b[1;31mHttpError\u001b[0m: Invalid bucket name: 'pinecone-datasets-dev\\wikipedia-simple-text-embedding-ada-002-100K', 400"
     ]
    }
   ],
   "source": [
    "# Run this cell\n",
    "\n",
    "import pinecone_datasets\n",
    "\n",
    "# We import a dataset\n",
    "dataset = pinecone_datasets.load_dataset('wikipedia-simple-text-embedding-ada-002-100K')\n",
    "# We drop sparse_values as they are not needed for this example\n",
    "dataset.documents.drop(['metadata'], axis=1, inplace=True)\n",
    "dataset.documents.rename(columns={'blob': 'metadata'}, inplace=True)\n",
    "# We will use rows of the dataset up to index 30_000 to make the upload to the Pinecone Vector Database faster\n",
    "dataset.documents.drop(dataset.documents.index[30_000:], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Run the cell bellow to check that your pinecone database does not contain any indexes, and delete them if there are any.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pinecone.list_indexes()\n",
    "\n",
    "#pinecone.delete_index(\"index_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Use the [`create_index`](https://docs.pinecone.io/reference/create_index) function to create an index, name it `langchain-retrieval-augmentation-fast`, set the dimension to **1536**, and use the metric `cosine`.\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "**Note:** We set the dimension to 1536 as this is the dimension of OpenAI's text embedding model 'text-embedding-ada-002' that we use to embed the data. If you wish to use another model (for instance a Hugging Face model using HuggingFaceInferenceAPIEmbeddings), you will have to change this number to match the dimension of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   Check that your pinecone database does not contain any indexes, and delete them if there are any.\n",
    "#\n",
    "###########################\n",
    "\n",
    "index_name = 'langchain-retrieval-augmentation-fast'\n",
    "\n",
    "# TODO : your code bellow\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Run the cell bellow to push the data on to the Vector Database. This can take a few minutes.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell - It may take a few minutes\n",
    "\n",
    "import time\n",
    "\n",
    "index = pinecone.GRPCIndex(index_name) # GRPC allows for faster upserts to the Pinecone Vector Database\n",
    "# wait a moment for the index to be fully initialized\n",
    "time.sleep(1)\n",
    "\n",
    "index.describe_index_stats()\n",
    "\n",
    "for batch in dataset.iter_documents(batch_size=100):\n",
    "    index.upsert(batch)\n",
    "\n",
    "\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Creating a vector store\n",
    "\n",
    "Now that we've build our index we can switch over to LangChain. We need to initialize a LangChain vector store using the same index we just built. For this we will also need a LangChain embedding object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Run the following cells to initialize a LangChain vector store.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "model_name = 'text-embedding-ada-002'\n",
    "\n",
    "embed = OpenAIEmbeddings(\n",
    "    model=model_name,\n",
    "    openai_api_key=OPENAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "\n",
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "text_field = \"text\"\n",
    "\n",
    "# switch back to normal index for langchain\n",
    "index = pinecone.Index(index_name)\n",
    "\n",
    "vectorstore = Pinecone(\n",
    "    index, embed, text_field\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Run the following cell to perform a `similarity_search` of the content of the query on our [vectorstore](https://python.langchain.com/docs/modules/data_connection/vectorstores/) containing embedded information about our augmentation dataset.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "\n",
    "query = \"When was Napoleon born\"\n",
    "\n",
    "vectorstore.similarity_search(\n",
    "    query,  # our search query\n",
    "    k=3  # return 3 most relevant docs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Augmented LLM query\n",
    "\n",
    "We finally have all the pieces of the puzzle, and we can now force the LLM to answer a question based on the information it is seeing being returned from the vectorstore. This allows for many things, like giving it up-to-date information, but can also be used to [cite sources](https://python.langchain.com/docs/use_cases/question_answering/vector_db_qa#return-source-documents). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Run the following cell to create a `VectorStoreRetrieverMemory` object that we will pass on to the LLMChain.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "from langchain.memory.vectorstore import VectorStoreRetrieverMemory\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs=dict(k=1))\n",
    "memory_RAG = VectorStoreRetrieverMemory(retriever=retriever)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Create a new prompt template with a question of your choice. For example, you can ask about the life of historical figures.\n",
    "\n",
    "- Using langchain.LLMChain, create a chain to run the new prompt template by our OpenAI LLM davinci. Add to the lists of arguments `memory = memory_RAG` to instruct the LLM to look for the answer in the vectorstore.\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   Asks the language model to give the date of birth of a historical figure.\n",
    "#\n",
    "###########################\n",
    "\n",
    "# TODO : your code bellow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Augmented LLM query with sources\n",
    "\n",
    "As stated before, we can use retrieval augmentation to provide sources to the output of the LLM (provided the sources were in the metadata of the embedded dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "chain = RetrievalQAWithSourcesChain.from_chain_type(llm = davinci, chain_type=\"stuff\", retriever=retriever)\n",
    "chain({\"question\": \"Tell me about the life of Napoleon\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>üë©‚Äçüíªüë®‚Äçüíª Optional action</b>\n",
    "\n",
    "- Reading: [Making Retrieval Augmented Generation Fast](https://www.pinecone.io/learn/fast-retrieval-augmented-generation/)\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. The end\n",
    "\n",
    "That's the end of this lab! We hope you learned a lot through it, and that you are now ready to go on the adventure on your own and explore all that is possible to do with LLMs and vector Databases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>üë©‚Äçüíªüë®‚Äçüíª Optional action</b>\n",
    "\n",
    "Explore the other exemple of applications of vector databases listed in Pinecone's documentation:\n",
    "\n",
    "https://docs.pinecone.io/page/examples\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To submit this assignment and **every other future assignment**, included the **final assignment** you have to:\n",
    "- Commit and push your code to GitHub\n",
    "- Go to **your** repository of the assignment. This must be on our course organisation `UCL-ELEC0136` and usually has the pattern `https://github.com/UCL-ELEC0136/<assignment-name>-<your-github-username>`.\n",
    "- Go in the `Pull requests` tab and click on the `Feedback` pull request.\n",
    "- Click on `Files changed` and verify that the files you have changed are listed.\n",
    "- Merge the pull request by clicking on `Merge pull request` and then `Confirm merge`.\n",
    "\n",
    "We are now ready to push our code that acquires data from GitHub to our repository (which is also GitHub, but this is just a coincidence, we could have used any other API, like Twitter's or Facebook's).\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "Submit your assignment by following the steps above.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
